{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5OsZNBtINMamByhDm2+7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GENTLEW1ND/Python/blob/main/Building_Support_Vector_Machine_Classifier_from_Scratch_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM Classifier**"
      ],
      "metadata": {
        "id": "xCWq7bLQH_ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equation of the Hyperplace:\n",
        "\n",
        "**y = wx - b**"
      ],
      "metadata": {
        "id": "cFkXhsjiIDPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent\n",
        "\n",
        "Gradient Descent is an optimization alogrithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n",
        "\n",
        "w = w - a*dw\n",
        "\n",
        "b = b - a*db"
      ],
      "metadata": {
        "id": "T0fCac1yIMmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate\n",
        "\n",
        "Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."
      ],
      "metadata": {
        "id": "IqnHLzwlIhZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xli1Bpj_H9oS"
      },
      "outputs": [],
      "source": [
        "# Importing the numpy the library\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine Classifier"
      ],
      "metadata": {
        "id": "3AWAJv1MKj1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SVM_classifier():\n",
        "\n",
        "  # initiating the hyperparameters\n",
        "  def __init__(self, learning_rate, no_of_iteration, lamda_parameter):\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iteration = no_of_iteration\n",
        "    self.lamda_parameter = lamda_parameter\n",
        "\n",
        "  # fitting the dataset to SVM Classifier\n",
        "  def fit(self, X, Y):\n",
        "\n",
        "    # m -> number of datapoints or number of rows\n",
        "    # n -> number of featurers or number of columns\n",
        "    self.m, self.n = X.shape\n",
        "\n",
        "    # initiating the weight values and the bias value\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "    # implementing the Gradient Descent Algorithm for optimization\n",
        "    for i in range(self.no_of_iteration):\n",
        "      self.update_weights()\n",
        "\n",
        "  # function for updating the weight and bias value\n",
        "  def update_weights(self):\n",
        "\n",
        "    # Label encoding\n",
        "    y_label = np.where(self.Y <= 0, -1, 1)\n",
        "\n",
        "\n",
        "    # gradients ( dw, db )\n",
        "    for index, x_i in enumerate(self.X):\n",
        "\n",
        "      condition = y_label[index] * (np.dot(x_i,self.w) - self.b) >= 1\n",
        "\n",
        "      if(condition == True):\n",
        "\n",
        "        dw = 2 * self.lamda_parameter * self.w\n",
        "        db = 0\n",
        "\n",
        "      else:\n",
        "\n",
        "        dw = 2 * self.lamda_parameter * self.w - np.dot(x_i, y_label[index])\n",
        "        db = y_label[index]\n",
        "\n",
        "      # gradient descent\n",
        "      self.w = self.w - self.learning_rate * dw\n",
        "\n",
        "      self.b = self.b - self.learning_rate * db\n",
        "\n",
        "  # predict the label  for a given input value\n",
        "  def predict(self, X):\n",
        "\n",
        "    # Equation of a hyperplane\n",
        "    output = np.dot(X, self.w) - self.b\n",
        "\n",
        "    predicted_labels = np.sign(output)\n",
        "\n",
        "    y_hat = np.where(predicted_labels <= -1, 0, 1)\n",
        "\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "8Od2DbRQKd6Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQZLe25SbP00"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}